{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install beautifulsoup4 pandas html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade html5lib lxml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "class AuditReportParser:\n",
    "    def parse_html(self, file_path):\n",
    "        \"\"\"\n",
    "        HTML 파일을 읽고 BeautifulSoup 객체로 파싱하여 반환합니다.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): HTML 파일 경로.\n",
    "            \n",
    "        Returns:\n",
    "            BeautifulSoup: 파싱된 HTML 객체.\n",
    "            \n",
    "        Raises:\n",
    "            IOError: 파일을 읽을 수 없을 때 발생.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 감사보고서 파일의 인코딩이 'euc-kr'인 경우가 많습니다.\n",
    "            with open(file_path, 'r', encoding='euc-kr') as f:\n",
    "                html_content = f.read()\n",
    "            return BeautifulSoup(html_content, 'html.parser')\n",
    "        except IOError as e:\n",
    "            print(f\"파일을 읽는 중 오류가 발생했습니다: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_sections(self, parsed_html):\n",
    "        \"\"\"\n",
    "        HTML에서 <p> 태그의 텍스트를 문단별로 추출하여 리스트로 반환합니다.\n",
    "        \n",
    "        Args:\n",
    "            parsed_html (BeautifulSoup): parse_html() 메서드가 반환한 객체.\n",
    "            \n",
    "        Returns:\n",
    "            list: 각 <p> 태그의 텍스트가 담긴 문자열 리스트.\n",
    "        \"\"\"\n",
    "        if not parsed_html:\n",
    "            return []\n",
    "            \n",
    "        paragraphs = parsed_html.find_all('p')\n",
    "        parsed_texts = [p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)]\n",
    "        \n",
    "        return parsed_texts\n",
    "    \n",
    "    def extract_tables(self, parsed_html):\n",
    "        \"\"\"\n",
    "        HTML에서 class=\"TABLE\" 속성을 가진 테이블을 DataFrame 리스트로 추출합니다.\n",
    "        \n",
    "        Args:\n",
    "            parsed_html (BeautifulSoup): parse_html() 메서드가 반환한 객체.\n",
    "            \n",
    "        Returns:\n",
    "            list: 각 테이블을 변환한 pandas DataFrame 객체 리스트.\n",
    "        \"\"\"\n",
    "        if not parsed_html:\n",
    "            return []\n",
    "            \n",
    "        tables = parsed_html.find_all('table', class_='TABLE')\n",
    "        dfs = []\n",
    "        \n",
    "        for table in tables:\n",
    "            rows = table.find_all('tr')\n",
    "            \n",
    "            if not rows:\n",
    "                continue\n",
    "                \n",
    "            data = []\n",
    "            max_cols = 0\n",
    "            for row in rows:\n",
    "                cols = row.find_all(['td', 'th'])\n",
    "                row_data = [col.get_text(strip=True) for col in cols]\n",
    "                data.append(row_data)\n",
    "                if len(row_data) > max_cols:\n",
    "                    max_cols = len(row_data)\n",
    "            \n",
    "            padded_data = [row + [''] * (max_cols - len(row)) for row in data]\n",
    "            \n",
    "            if padded_data and len(padded_data) > 1:\n",
    "                df = pd.DataFrame(padded_data[1:], columns=padded_data[0])\n",
    "                dfs.append(df)\n",
    "            elif padded_data:\n",
    "                df = pd.DataFrame(padded_data)\n",
    "                dfs.append(df)\n",
    "        \n",
    "        return dfs\n",
    "\n",
    "    def normalize_text(self, text):\n",
    "        \"\"\"\n",
    "        금융 텍스트를 정규화하고 정제합니다.\n",
    "        \n",
    "        Args:\n",
    "            text (str): 정제할 텍스트.\n",
    "            \n",
    "        Returns:\n",
    "            str: 정규화된 텍스트.\n",
    "        \"\"\"\n",
    "        # 불필요한 공백 문자(줄바꿈, 탭 등)를 단일 공백으로 치환\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # 괄호와 그 안의 내용을 제거 (예: (주)삼성전자)\n",
    "        text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "        # 특수문자 제거 (한글, 영어, 숫자, 공백 제외)\n",
    "        text = re.sub(r'[^가-힣a-zA-Z0-9\\s.,]', '', text)\n",
    "        # 쉼표 뒤에 공백 추가\n",
    "        text = re.sub(r',', ', ', text)\n",
    "        # 여러 개의 공백을 단일 공백으로\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def extract_inventory_table(self, parsed_html):\n",
    "        \"\"\"\n",
    "        HTML에서 '재고자산 내역' 테이블을 찾아 DataFrame으로 반환합니다.\n",
    "        \"\"\"\n",
    "        search_texts = [\n",
    "            '보고기간종료일 현재 재고자산의 내역은 다음과 같습니다.',\n",
    "            '재고자산의 내역은 다음과 같습니다.',\n",
    "        ]\n",
    "\n",
    "        for text in search_texts:\n",
    "            text_elements = parsed_html.find_all(lambda tag: tag.name in ['p', 'span'] and text in tag.get_text(strip=True))\n",
    "\n",
    "            for element in text_elements:\n",
    "                next_table = element.find_next('table', class_='TABLE')\n",
    "\n",
    "                if next_table:\n",
    "                    # extract_tables 메서드의 로직을 재사용하여 DataFrame으로 변환\n",
    "                    rows = next_table.find_all('tr')\n",
    "                    data = []\n",
    "                    max_cols = 0\n",
    "                    for row in rows:\n",
    "                        cols = row.find_all(['td', 'th'])\n",
    "                        row_data = [col.get_text(strip=True) for col in cols]\n",
    "                        data.append(row_data)\n",
    "                        if len(row_data) > max_cols:\n",
    "                            max_cols = len(row_data)\n",
    "\n",
    "                    padded_data = [row + [''] * (max_cols - len(row)) for row in data]\n",
    "\n",
    "                    if padded_data and len(padded_data) >= 3:\n",
    "                        # 임시 DataFrame 생성 (헤더 없이 데이터만 가져옴)\n",
    "                        df = pd.DataFrame(padded_data[2:])\n",
    "\n",
    "                        # 하드코딩된 상위 헤더\n",
    "                        full_upper_header = ['구분', '당기말', '당기말', '당기말', '전기말', '전기말', '전기말']\n",
    "\n",
    "                        # 하위 헤더를 HTML에서 가져옴\n",
    "                        lower_header_raw = padded_data[1]\n",
    "\n",
    "                        # 하위 헤더를 상위 헤더의 길이와 맞추기\n",
    "                        # '구분' 헤더에 해당하는 빈 문자열을 추가\n",
    "                        full_lower_header = [''] + lower_header_raw\n",
    "\n",
    "                        # MultiIndex 생성 및 DataFrame에 적용\n",
    "                        df.columns = pd.MultiIndex.from_tuples(list(zip(full_upper_header, full_lower_header)))\n",
    "\n",
    "                        # '구분' 열을 인덱스로 설정\n",
    "                        # MultiIndex에서 '구분' 열은 ('구분', '') 튜플로 접근해야 함\n",
    "                        #df.set_index(('구분', ''), inplace=True)\n",
    "                        df = df.drop('전기말', axis=1, level=0)\n",
    "                        return df\n",
    "                    elif padded_data:\n",
    "                        # 헤더가 1개인 경우\n",
    "                        return pd.DataFrame(padded_data[1:], columns=padded_data[0])\n",
    "\n",
    "        return None\n",
    "\n",
    "    def extract_investment_changes(self, parsed_html):\n",
    "        \"\"\"\n",
    "        HTML에서 '종속기업, 관계기업 및 공동기업 투자의 변동내역' 테이블을 찾아 DataFrame으로 반환합니다.\n",
    "        \n",
    "        Args:\n",
    "            parsed_html (BeautifulSoup): 파싱된 HTML 객체.\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: 변동내역 데이터가 담긴 DataFrame.\n",
    "        \"\"\"\n",
    "        # 텍스트를 포함하는 p 또는 span 태그를 찾습니다.\n",
    "        search_text = '가. 당기 및 전기 중 종속기업, 관계기업 및 공동기업 투자의 변동'\n",
    "        \n",
    "        title_element = parsed_html.find(\n",
    "            lambda tag: tag.name in ['p', 'span'] and search_text in tag.get_text(strip=True)\n",
    "        )\n",
    "\n",
    "        if not title_element:\n",
    "            print(f\"'{search_text}' 텍스트를 찾을 수 없습니다.\")\n",
    "            return None\n",
    "        \n",
    "        # 텍스트 다음에 나오는 첫 번째 'TABLE' 클래스 테이블을 찾습니다.\n",
    "        table_element = title_element.find_next('table', class_='TABLE')\n",
    "\n",
    "        if not table_element:\n",
    "            print(\"관련 테이블을 찾을 수 없습니다.\")\n",
    "            return None\n",
    "        \n",
    "        # 테이블 데이터 추출\n",
    "        rows = table_element.find_all('tr')\n",
    "        \n",
    "        if len(rows) < 2:\n",
    "            print(\"테이블에 충분한 데이터가 없습니다.\")\n",
    "            return None\n",
    "            \n",
    "        # 헤더와 데이터 추출\n",
    "        headers = [th.get_text(strip=True) for th in rows[0].find_all(['th', 'td'])]\n",
    "        data = [[td.get_text(strip=True) for td in row.find_all('td')] for row in rows[1:]]\n",
    "\n",
    "        # DataFrame 생성 및 반환\n",
    "        df = pd.DataFrame(data, columns=headers)\n",
    "        if '전기' in df.columns:\n",
    "            df = df.drop(columns=['전기'])\n",
    "        \n",
    "        return df\n",
    "    def extract_major_investments(self, parsed_html):\n",
    "        \"\"\"\n",
    "        HTML에서 '주요 관계기업 및 공동기업 투자 현황' 테이블을 찾아 DataFrame으로 반환합니다.\n",
    "        \n",
    "        Args:\n",
    "            parsed_html (BeautifulSoup): 파싱된 HTML 객체.\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: 주요 투자 현황 데이터가 담긴 DataFrame.\n",
    "        \"\"\"\n",
    "        search_texts = [\n",
    "            '(1) 관계기업 투자',\n",
    "            '투자 현황은 다음과 같습니다'\n",
    "        ]\n",
    "        \n",
    "        for text in search_texts:\n",
    "            title_element = parsed_html.find(\n",
    "                lambda tag: tag.name in ['p', 'span'] and text in tag.get_text(strip=True)\n",
    "            )\n",
    "            \n",
    "            if title_element:\n",
    "                table_element = title_element.find_next('table', class_='TABLE')\n",
    "                if table_element:\n",
    "                    rows = table_element.find_all('tr')\n",
    "                    if len(rows) < 2:\n",
    "                        continue\n",
    "                        \n",
    "                    headers = [th.get_text(strip=True) for th in rows[0].find_all(['th', 'td'])]\n",
    "                    data = [[td.get_text(strip=True) for td in row.find_all('td')] for row in rows[1:]]\n",
    "                    \n",
    "                    df = pd.DataFrame(data, columns=headers)\n",
    "                    \n",
    "                    # 지분율 컬럼 전처리\n",
    "                    if '지분율(%)' in df.columns:\n",
    "                        df['지분율(%)'] = pd.to_numeric(df['지분율(%)'], errors='coerce')\n",
    "                    \n",
    "                    return df\n",
    "        \n",
    "        print(\"주요 관계기업 투자 현황 테이블을 찾을 수 없습니다.\")\n",
    "        return None\n",
    "\n",
    "    def extract_subsidiaries(self, parsed_html):\n",
    "        \"\"\"\n",
    "        HTML에서 '(1) 주요 종속기업' 테이블을 찾아 두 가지 유형의 헤더를 모두 처리하여 DataFrame으로 반환합니다.\n",
    "        \"\"\"\n",
    "        search_text = '(1) 주요 종속기업'\n",
    "        \n",
    "        title_element = parsed_html.find(\n",
    "            lambda tag: tag.name in ['p', 'span'] and search_text in tag.get_text(strip=True)\n",
    "        )\n",
    "\n",
    "        if not title_element:\n",
    "            return None\n",
    "        \n",
    "        table_element = title_element.find_next('table', class_='TABLE')\n",
    "        \n",
    "        if not table_element:\n",
    "            return None\n",
    "        \n",
    "        rows = table_element.find_all('tr')\n",
    "        \n",
    "        if len(rows) < 2:\n",
    "            return None\n",
    "            \n",
    "        # 첫 번째 행의 'rowspan' 속성으로 다중 헤더 여부 판단\n",
    "        first_row_ths = rows[0].find_all(['th', 'td'])\n",
    "        is_multi_header = any(th.get('rowspan') for th in first_row_ths)\n",
    "\n",
    "        # 헤더와 데이터 추출\n",
    "        if is_multi_header and len(rows) >= 2:\n",
    "            # 다중 헤더 처리: 두 번째 행의 헤더를 사용\n",
    "            # 첫 번째 행은 버리고 두 번째 행을 헤더로 사용\n",
    "            lower_header_elements = rows[1].find_all(['th', 'td'])\n",
    "            headers = [th.get_text(strip=True) for th in lower_header_elements]\n",
    "            \n",
    "            # 첫 번째 열에 해당하는 헤더가 없으면 '기업명'으로 채움\n",
    "            if len(headers) != len(rows[2].find_all('td')):\n",
    "                headers = ['기업명'] + [h for h in headers]\n",
    "                \n",
    "            data = [[td.get_text(strip=True) for td in row.find_all('td')] for row in rows[2:]]\n",
    "            \n",
    "            df = pd.DataFrame(data, columns=headers)\n",
    "            \n",
    "        else:\n",
    "            # 단일 헤더 처리\n",
    "            headers = [th.get_text(strip=True) for th in rows[0].find_all(['th', 'td'])]\n",
    "            data = [[td.get_text(strip=True) for td in row.find_all('td')] for row in rows[1:]]\n",
    "            \n",
    "            df = pd.DataFrame(data, columns=headers)\n",
    "            \n",
    "        # 공통 전처리: 불필요한 텍스트 및 기호 제거\n",
    "        df.columns = df.columns.astype(str).str.replace(r'\\(.*?\\)', '', regex=True).str.strip()\n",
    "        df.rename(columns={'기업명': '기업명', '당기순이익(손실)': '당기순이익'}, inplace=True)\n",
    "        \n",
    "        # 숫자형 컬럼 전처리 (콤마와 하이픈 제거 후 float 변환)\n",
    "        for col in df.columns[1:]:\n",
    "            df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', '').str.replace('-', '0'), errors='coerce')\n",
    "\n",
    "        return df\n",
    "    def _extract_table_by_text(self, parsed_html, search_text):\n",
    "        \"\"\"\n",
    "        특정 텍스트 다음에 오는 단일 헤더 테이블을 파싱합니다.\n",
    "        \"\"\"\n",
    "        title_element = parsed_html.find(\n",
    "            lambda tag: tag.name in ['p', 'span'] and search_text in tag.get_text(strip=True)\n",
    "        )\n",
    "        if not title_element:\n",
    "            return None\n",
    "        \n",
    "        table_element = title_element.find_next('table', class_='TABLE')\n",
    "        if not table_element:\n",
    "            return None\n",
    "            \n",
    "        rows = table_element.find_all('tr')\n",
    "        if len(rows) < 2:\n",
    "            return None\n",
    "        \n",
    "        headers = [th.get_text(strip=True) for th in rows[0].find_all(['th', 'td'])]\n",
    "        data = [[td.get_text(strip=True) for td in row.find_all('td')] for row in rows[1:]]\n",
    "        \n",
    "        return pd.DataFrame(data, columns=headers)\n",
    "    \n",
    "    def _extract_single_financial_table(self, parsed_html, search_text):\n",
    "        \"\"\"\n",
    "        특정 텍스트 다음에 오는 테이블을 파싱합니다.\n",
    "        \"\"\"\n",
    "        title_element = parsed_html.find(\n",
    "            lambda tag: tag.name in ['p', 'span', 'td'] and search_text in tag.get_text(strip=True)\n",
    "        )\n",
    "        if not title_element:\n",
    "            return None\n",
    "        \n",
    "        table_element = title_element.find_next('table', class_='TABLE')\n",
    "        if not table_element:\n",
    "            table_element = title_element.find_parent('table', class_='TABLE')\n",
    "            if not table_element:\n",
    "                return None\n",
    "        \n",
    "        rows = table_element.find_all('tr')\n",
    "        if len(rows) < 2:\n",
    "            return None\n",
    "        \n",
    "        header_row_index = -1\n",
    "        for i, row in enumerate(rows):\n",
    "            if any('구분' in th.get_text(strip=True) for th in row.find_all(['th', 'td'])):\n",
    "                header_row_index = i\n",
    "                break\n",
    "\n",
    "        if header_row_index == -1:\n",
    "            return None\n",
    "\n",
    "        headers = [th.get_text(strip=True) for th in rows[header_row_index].find_all(['th', 'td'])]\n",
    "        data = [[td.get_text(strip=True) for td in row.find_all('td')] for row in rows[header_row_index+1:]]\n",
    "        \n",
    "        df = pd.DataFrame(data, columns=headers)\n",
    "        \n",
    "        df = df[~df['구분'].astype(str).str.contains('요약', na=False)].copy()\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _extract_sub_table(self, parsed_html, table_title):\n",
    "        \"\"\"\n",
    "        특정 제목을 가진 하위 테이블을 파싱하는 도우미 함수.\n",
    "        \"\"\"\n",
    "        title_element = parsed_html.find(\n",
    "            lambda tag: tag.name in ['td'] and table_title in tag.get_text(strip=True)\n",
    "        )\n",
    "        if not title_element:\n",
    "            return None\n",
    "            \n",
    "        table_element = title_element.find_parent('table', class_='TABLE')\n",
    "        if not table_element:\n",
    "            return None\n",
    "\n",
    "        rows = table_element.find_all('tr')\n",
    "        if not rows or len(rows) < 2:\n",
    "            return None\n",
    "        \n",
    "        headers = [th.get_text(strip=True) for th in rows[0].find_all(['th', 'td'])]\n",
    "        data = [[td.get_text(strip=True) for td in row.find_all('td')] for row in rows[1:]]\n",
    "        df = pd.DataFrame(data, columns=headers)\n",
    "        \n",
    "        return df   \n",
    "\n",
    "    def _parse_table_content(self, table_element):\n",
    "        \"\"\"BeautifulSoup 테이블 객체를 DataFrame으로 파싱하는 도우미 함수.\"\"\"\n",
    "        if not table_element:\n",
    "            return None\n",
    "        \n",
    "        rows = table_element.find_all('tr')\n",
    "        if not rows or len(rows) < 2:\n",
    "            return None\n",
    "        \n",
    "        headers = [th.get_text(strip=True) for th in rows[0].find_all(['th', 'td'])]\n",
    "        data = [[td.get_text(strip=True) for td in row.find_all('td')] for row in rows[1:]]\n",
    "        \n",
    "        return pd.DataFrame(data, columns=headers)\n",
    "    \n",
    "    def _parse_sub_table_by_title(self, parsed_html, table_title):\n",
    "        \"\"\"특정 제목을 가진 하위 테이블을 파싱하는 도우미 함수.\"\"\"\n",
    "        title_element = parsed_html.find(\n",
    "            lambda tag: tag.name in ['td'] and table_title in tag.get_text(strip=True)\n",
    "        )\n",
    "        if not title_element:\n",
    "            return None\n",
    "            \n",
    "        table_element = title_element.find_parent('table', class_='TABLE')\n",
    "        if not table_element:\n",
    "            return None\n",
    "\n",
    "        rows = table_element.find_all('tr')\n",
    "        if not rows or len(rows) < 2:\n",
    "            return None\n",
    "        \n",
    "        headers = [th.get_text(strip=True) for th in rows[0].find_all(['th', 'td'])]\n",
    "        data = [[td.get_text(strip=True) for td in row.find_all('td')] for row in rows[1:]]\n",
    "        \n",
    "        return pd.DataFrame(data, columns=headers)\n",
    "    def _parse_table_content(self, table_element):\n",
    "        \"\"\"BeautifulSoup 테이블 객체를 DataFrame으로 파싱하는 도우미 함수.\"\"\"\n",
    "        if not table_element:\n",
    "            return None\n",
    "        rows = table_element.find_all('tr')\n",
    "        if not rows or len(rows) < 2:\n",
    "            return None\n",
    "        headers = [th.get_text(strip=True) for th in rows[0].find_all(['th', 'td'])]\n",
    "        data = [[td.get_text(strip=True) for td in row.find_all('td')] for row in rows[1:]]\n",
    "        return pd.DataFrame(data, columns=headers)\n",
    "\n",
    "        \n",
    "    def extract_financial_info_double(self, parsed_html):\n",
    "        \"\"\"관계기업 재무정보 테이블을 파싱하여 DataFrame으로 반환합니다.\"\"\"\n",
    "        search_pattern_main = re.compile(r'\\(2\\)\\s*주요\\s*관계기업')\n",
    "        title_element = parsed_html.find(\n",
    "            lambda tag: tag.name in ['p', 'span'] and re.search(search_pattern_main, tag.get_text(strip=True))\n",
    "        )\n",
    "        if not title_element:\n",
    "            print(\"경고: '주요 관계기업' 제목을 찾을 수 없습니다.\")\n",
    "            return None\n",
    "            \n",
    "        # 두 개의 테이블이 연속해서 있는지 확인하는 로직\n",
    "        tables_after_title = title_element.find_all_next('table', class_='TABLE', limit=2)\n",
    "        is_two_tables_case = len(tables_after_title) >= 2\n",
    "\n",
    "        if is_two_tables_case:\n",
    "            table_element_1 = tables_after_title[0]\n",
    "            table_element_2 = tables_after_title[1]\n",
    "\n",
    "            if not table_element_1 or not table_element_2:\n",
    "                print(\"경고: 두 개의 분리된 테이블을 찾을 수 없습니다.\")\n",
    "                return None\n",
    "            \n",
    "            df_bs = self._parse_table_content(table_element_1)\n",
    "            df_is = self._parse_table_content(table_element_2)\n",
    "\n",
    "            if df_bs is None or df_is is None:\n",
    "                return None\n",
    "            \n",
    "            samsung_card_col_name = next((col for col in df_bs.columns if '삼성카드' in col), None)\n",
    "            if samsung_card_col_name and '유동자산' in df_bs['구분'].values and '비유동자산' in df_bs['구분'].values:\n",
    "                print(\"삼성카드 데이터 보정 중...\")\n",
    "                df_bs.loc[df_bs['구분'] == '비유동자산', samsung_card_col_name] = df_bs.loc[df_bs['구분'] == '유동자산', samsung_card_col_name].values[0]\n",
    "                df_bs.loc[df_bs['구분'] == '비유동부채', samsung_card_col_name] = df_bs.loc[df_bs['구분'] == '유동부채', samsung_card_col_name].values[0]\n",
    "            \n",
    "            df_bs.set_index('구분', inplace=True)\n",
    "            df_is.set_index('구분', inplace=True)\n",
    "            final_df = pd.concat([df_bs, df_is], axis=0, join='outer')\n",
    "            final_df.index.name = None\n",
    "            final_df = final_df.T\n",
    "            \n",
    "            final_df.loc[:, '순자산'] = pd.NA\n",
    "            final_df.index = final_df.index.str.replace(r'[\\s\\(\\)\\*]', '', regex=True)\n",
    "            \n",
    "            #for col in final_df.columns:\n",
    "                #final_df[col] = pd.to_numeric(final_df[col].astype(str).str.replace(',', '', regex=False).str.replace('-', '0', regex=False).str.replace('(', '-', regex=False).str.replace(')', '', regex=False), errors='coerce')\n",
    "            \n",
    "            return final_df\n",
    "\n",
    "        \n",
    "    def extract_financial_info_single(self, parsed_html):\n",
    "        \"\"\"\n",
    "        통합된 '관계기업의 재무정보' 테이블을 파싱하여 DataFrame으로 반환합니다.\n",
    "        \"\"\"\n",
    "        search_pattern = re.compile(r'\\(2\\)\\s*주요\\s*관계기업')\n",
    "        \n",
    "        title_element = parsed_html.find(\n",
    "            lambda tag: tag.name in ['p', 'span'] and re.search(search_pattern, tag.get_text(strip=True))\n",
    "        )\n",
    "        if not title_element:\n",
    "            print(\"경고: '주요 관계기업' 제목을 찾을 수 없습니다.\")\n",
    "            return None\n",
    "        \n",
    "        table_element = title_element.find_next('table', class_='TABLE')\n",
    "        if not table_element:\n",
    "            print(\"경고: 관련 테이블을 찾을 수 없습니다.\")\n",
    "            return None\n",
    "            \n",
    "        rows = table_element.find_all('tr') \n",
    "        if len(rows) < 2:\n",
    "            print(\"경고: 테이블에 충분한 행이 없습니다.\")\n",
    "            return None\n",
    "\n",
    "        # 헤더 행의 개수를 기준으로 파싱 로직 분기\n",
    "        if len(rows) >= 3 and len(rows[1].find_all(['th', 'td'])) > 1:\n",
    "            # 헤더 행이 2개인 경우\n",
    "            company_names_row = rows[1]\n",
    "            data_start_row = 2\n",
    "        else:\n",
    "            # 헤더 행이 1개인 경우\n",
    "            company_names_row = rows[0]\n",
    "            data_start_row = 1\n",
    "        \n",
    "        # 1. 기업명(컬럼) 추출: <th>와 <td> 태그를 모두 찾아 '구분'을 제외\n",
    "        company_names = []\n",
    "        for elem in company_names_row.find_all(['th', 'td']):\n",
    "            elem_text = elem.get_text(strip=True)\n",
    "            if '구분' not in elem_text:\n",
    "                company_names.append(elem_text)\n",
    "        \n",
    "        # 2. 데이터 및 재무 항목(인덱스) 추출\n",
    "        financial_items = []\n",
    "        financial_data = []\n",
    "\n",
    "        clean_pattern = re.compile(r'[\\s\\(\\)\\*]')\n",
    "\n",
    "        for row in rows[data_start_row:]:\n",
    "            tds = row.find_all('td')\n",
    "            if '요약' in tds[0].get_text(strip=True):\n",
    "                continue\n",
    "            \n",
    "            item_name = re.sub(clean_pattern, '', tds[0].get_text(strip=True))\n",
    "            item_data = [td.get_text(strip=True) for td in tds[1:]]\n",
    "            \n",
    "            if len(item_data) != len(company_names):\n",
    "                print(f\"경고: 데이터 행의 열 개수({len(item_data)})와 기업명 개수({len(company_names)})가 불일치합니다.\")\n",
    "                continue\n",
    "\n",
    "            financial_items.append(item_name)\n",
    "            financial_data.append(item_data)\n",
    "        \n",
    "        if not financial_data:\n",
    "            print(\"경고: 유효한 재무 데이터 행을 찾을 수 없습니다. DataFrame 생성 실패.\")\n",
    "            return None\n",
    "        \n",
    "        # 3. DataFrame 생성\n",
    "        df = pd.DataFrame(financial_data, columns=company_names, index=financial_items)\n",
    "\n",
    "        # 4. '순자산' 열 추가 (요청에 따라)\n",
    "        df.loc['순자산', :] = pd.NA\n",
    "        \n",
    "        # 5. 숫자형 데이터 전처리\n",
    "        for col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', '').str.replace('-', '0').str.replace('(', '-').str.replace(')', ''), errors='coerce')\n",
    "        \n",
    "        # 최종적으로 행과 열을 뒤집어 원하는 형태로 만듭니다.\n",
    "        df = df.T\n",
    "        \n",
    "        return df\n",
    "    def extract_financial_info(self, parsed_html, year):\n",
    "        \"\"\"\n",
    "        관계기업 재무정보 테이블의 구조를 파악하고 적절한 파싱 메서드를 호출합니다.\n",
    "        \"\"\"\n",
    "        if year >= 2019:\n",
    "            print(f\"{year}년: 통합 테이블로 간주하고 파싱을 진행합니다.\")\n",
    "            return self.extract_financial_info_single(parsed_html)\n",
    "        else:\n",
    "            print(f\"{year}년: 분리된 두 개의 테이블로 간주하고 파싱을 진행합니다.\")\n",
    "            return self.extract_financial_info_double(parsed_html)\n",
    "    def extract_specific_investment_table(self,parsed_html):\n",
    "        \"\"\"\n",
    "        첫 번째 헤더 행을 무시하고, 두 번째 행을 헤더로 사용하여\n",
    "        복합 헤더 테이블을 DataFrame으로 파싱합니다.\n",
    "        \n",
    "        Args:\n",
    "            parsed_html (BeautifulSoup): 파싱된 HTML 객체.\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: 파싱된 데이터가 담긴 DataFrame.\n",
    "        \"\"\"\n",
    "        search_text = \"관계기업 투자주식의 내역은 다음과 같습니다.\"\n",
    "    \n",
    "        # 텍스트를 포함하는 태그를 찾습니다.\n",
    "        title_element = parsed_html.find(\n",
    "            lambda tag: tag.name in ['p', 'span'] and search_text in tag.get_text(strip=True)\n",
    "        )\n",
    "        \n",
    "        if not title_element:\n",
    "            print(f\"'{search_text}' 텍스트를 찾을 수 없습니다.\")\n",
    "            return None\n",
    "        \n",
    "        table = title_element.find_next('table', class_='TABLE')\n",
    "        \n",
    "        if not table:\n",
    "            print(\"class='TABLE'인 테이블을 찾을 수 없습니다.\")\n",
    "            return None\n",
    "\n",
    "        # 모든 행을 추출합니다.\n",
    "        rows = table.find_all('tr')\n",
    "        \n",
    "        if len(rows) < 2:\n",
    "            print(\"테이블에 충분한 데이터가 없습니다.\")\n",
    "            return None\n",
    "        \n",
    "        # 두 번째 행을 헤더로 사용합니다.\n",
    "        # 첫 번째 행은 무시합니다.\n",
    "        headers = [th.get_text(strip=True) for th in rows[1].find_all(['th', 'td'])]\n",
    "        \n",
    "        # 세 번째 행부터 데이터로 사용합니다.\n",
    "        data = [[td.get_text(strip=True) for td in row.find_all('td')] for row in rows[2:]]\n",
    "        \n",
    "        # '구분'에 해당하는 첫 번째 헤더가 누락되었을 수 있으므로 추가합니다.\n",
    "        # 제공된 HTML 구조에서는 두 번째 행에 '구분' 헤더가 없으므로 수동으로 추가합니다.\n",
    "        headers = ['구분'] + headers\n",
    "        \n",
    "        # 데이터 행도 첫 번째 열을 추가해야 함 (만약 테이블 구조에 따라 필요하다면)\n",
    "        # 하지만 제공된 HTML에서는 데이터가 6개 열이므로 그대로 사용합니다.\n",
    "        \n",
    "        # 데이터프레임 생성\n",
    "        df = pd.DataFrame(data, columns=headers)\n",
    "        df = df.iloc[:, :-2]\n",
    "        return df\n",
    "    def extract_tangible_assets_table(self,parsed_html):\n",
    "        # 띄어쓰기를 허용하는 정규표현식 패턴\n",
    "        search_pattern = re.compile(r'유형자산의\\s*변동\\s*내역은\\s*다음과\\s*같습니다\\.')\n",
    "\n",
    "        # 정규표현식 패턴을 사용하여 텍스트가 포함된 태그를 찾습니다.\n",
    "        title_element = parsed_html.find(\n",
    "            lambda tag: tag.name in ['p', 'span'] and search_pattern.search(tag.get_text(strip=True))\n",
    "        )\n",
    "        \n",
    "        if not title_element:\n",
    "            print(f\"'{search_pattern}' 텍스트를 찾을 수 없습니다.\")\n",
    "            return None\n",
    "        \n",
    "        table_element = title_element.find_next('table', class_='TABLE')\n",
    "        \n",
    "        if not table_element:\n",
    "            print(\"class='TABLE'인 테이블을 찾을 수 없습니다.\")\n",
    "            return None\n",
    "\n",
    "        # 모든 행을 추출합니다.\n",
    "        rows = table_element.find_all('tr')\n",
    "        \n",
    "        if len(rows) < 2:\n",
    "            print(\"테이블에 충분한 데이터가 없습니다.\")\n",
    "            return None\n",
    "        \n",
    "        # 두 번째 행을 헤더로 사용합니다.\n",
    "        # 첫 번째 행은 무시합니다.\n",
    "        headers = [th.get_text(strip=True) for th in rows[0].find_all(['th', 'td'])]\n",
    "        \n",
    "        # 세 번째 행부터 데이터로 사용합니다.\n",
    "        data = [[td.get_text(strip=True) for td in row.find_all('td')] for row in rows[1:]]\n",
    "        \n",
    "        # '구분'에 해당하는 첫 번째 헤더가 누락되었을 수 있으므로 추가합니다.\n",
    "        # 제공된 HTML 구조에서는 두 번째 행에 '구분' 헤더가 없으므로 수동으로 추가합니다.\n",
    " #       headers = ['구분'] + headers\n",
    "        \n",
    "        # 데이터 행도 첫 번째 열을 추가해야 함 (만약 테이블 구조에 따라 필요하다면)\n",
    "        # 하지만 제공된 HTML에서는 데이터가 6개 열이므로 그대로 사용합니다.\n",
    "        \n",
    "        # 데이터프레임 생성\n",
    "        df = pd.DataFrame(data, columns=headers)\n",
    "        return df\n",
    "    def extract_intangible_assets_table(self,parsed_html):\n",
    "        # 띄어쓰기를 허용하는 정규표현식 패턴\n",
    "        search_pattern = re.compile(r'무형자산의\\s*변동\\s*내역은\\s*다음과\\s*같습니다\\.')\n",
    "\n",
    "        # 정규표현식 패턴을 사용하여 텍스트가 포함된 태그를 찾습니다.\n",
    "        title_element = parsed_html.find(\n",
    "            lambda tag: tag.name in ['p', 'span'] and search_pattern.search(tag.get_text(strip=True))\n",
    "        )\n",
    "        \n",
    "        if not title_element:\n",
    "            print(f\"'{search_pattern}' 텍스트를 찾을 수 없습니다.\")\n",
    "            return None\n",
    "        \n",
    "        table_element = title_element.find_next('table', class_='TABLE')\n",
    "        \n",
    "        if not table_element:\n",
    "            print(\"class='TABLE'인 테이블을 찾을 수 없습니다.\")\n",
    "            return None\n",
    "\n",
    "        # 모든 행을 추출합니다.\n",
    "        rows = table_element.find_all('tr')\n",
    "        \n",
    "        if len(rows) < 2:\n",
    "            print(\"테이블에 충분한 데이터가 없습니다.\")\n",
    "            return None\n",
    "        \n",
    "        # 두 번째 행을 헤더로 사용합니다.\n",
    "        # 첫 번째 행은 무시합니다.\n",
    "        headers = [th.get_text(strip=True) for th in rows[0].find_all(['th', 'td'])]\n",
    "        \n",
    "        # 세 번째 행부터 데이터로 사용합니다.\n",
    "        data = [[td.get_text(strip=True) for td in row.find_all('td')] for row in rows[1:]]\n",
    "        \n",
    "        # '구분'에 해당하는 첫 번째 헤더가 누락되었을 수 있으므로 추가합니다.\n",
    "        # 제공된 HTML 구조에서는 두 번째 행에 '구분' 헤더가 없으므로 수동으로 추가합니다.\n",
    " #       headers = ['구분'] + headers\n",
    "        \n",
    "        # 데이터 행도 첫 번째 열을 추가해야 함 (만약 테이블 구조에 따라 필요하다면)\n",
    "        # 하지만 제공된 HTML에서는 데이터가 6개 열이므로 그대로 사용합니다.\n",
    "        \n",
    "        # 데이터프레임 생성\n",
    "        df = pd.DataFrame(data, columns=headers)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 클래스 인스턴스 생성\n",
    "parser = AuditReportParser()\n",
    "\n",
    "# 1. HTML 파일 파싱\n",
    "file_path = '삼성전자_감사보고서_2014_2024/감사보고서_2014.htm'\n",
    "parsed_html = parser.parse_html(file_path)\n",
    "\n",
    "if parsed_html:\n",
    "    # 2. 섹션별 텍스트 추출\n",
    "    sections = parser.extract_sections(parsed_html)\n",
    "    print(\"--- 추출된 문단 정보 (일부) ---\")\n",
    "    for i, section in enumerate(sections[:5]): # 상위 5개 문단만 출력\n",
    "        normalized_section = parser.normalize_text(section)\n",
    "        print(f\"[{i+1}] {normalized_section}\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    # 3. 테이블 데이터 추출\n",
    "    tables = parser.extract_tables(parsed_html)\n",
    "    print(\"--- 추출된 테이블 정보 (첫 번째 테이블) ---\")\n",
    "    if tables:\n",
    "        print(tables[0].head())\n",
    "    else:\n",
    "        print(\"파싱된 테이블이 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if parsed_html:\n",
    "    # 2. 섹션별 텍스트 추출 및 정규화\n",
    "    sections = parser.extract_sections(parsed_html)\n",
    "    print(f\"총 {len(sections)}개의 문단이 파싱되었습니다.\")\n",
    "    \n",
    "    print(\"\\n--- 파싱된 문단 (일부) ---\")\n",
    "    for i, section in enumerate(sections[:10]):\n",
    "        normalized_section = parser.normalize_text(section)\n",
    "        print(f\"[{i+1}] {normalized_section}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def browse_tables(file_path):\n",
    "    parser = AuditReportParser()\n",
    "    parsed_html = parser.parse_html(file_path)        \n",
    "    if not parsed_html:\n",
    "        return []\n",
    "    tables = parser.extract_tables(parsed_html)            \n",
    "    return tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 경로 (사용자 로컬 환경에 맞춰 경로를 조정해야 합니다)\n",
    "file_path = '삼성전자_감사보고서_2014_2024/감사보고서_2014.htm'\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    # 함수를 호출하여 모든 테이블을 추출합니다.\n",
    "    all_tables_2014 = browse_tables(file_path)\n",
    "\n",
    "    print(f\"파일: {file_path}\")\n",
    "    print(f\"총 {len(all_tables_2014)}개의 테이블이 파싱되었습니다.\")\n",
    "    \n",
    "    # 추출된 테이블들을 함수 외부에서 순차적으로 출력합니다.\n",
    "    for i, df in enumerate(all_tables_2014):\n",
    "        print(f\"\\n--- [테이블 {i+1}] ---\")\n",
    "        display(df)\n",
    "else:\n",
    "    print(f\"파일을 찾을 수 없습니다: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2014년부터 2024년까지 반복하여 11번째 테이블 추출 및 출력\n",
    "for year in range(2014, 2025):\n",
    "    file_path = f'삼성전자_감사보고서_2014_2024/감사보고서_{year}.htm'\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        all_tables_for_year = browse_tables(file_path)\n",
    "        \n",
    "        # 11번째 테이블(인덱스 10)이 존재하는지 확인\n",
    "        if len(all_tables_for_year) > 10:\n",
    "            selected_table = all_tables_for_year[10]\n",
    "            print(f\"\\n==================== {year}년 감사보고서의 11번째 테이블 ====================\")\n",
    "            display(selected_table)\n",
    "        else:\n",
    "            print(f\"\\n==================== {year}년 감사보고서 ====================\")\n",
    "            print(f\"총 {len(all_tables_for_year)}개의 테이블이 파싱되었으며, 11번째 테이블이 없습니다.\")\n",
    "    else:\n",
    "        print(f\"\\n파일을 찾을 수 없습니다: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 클래스 인스턴스 생성\n",
    "parser = AuditReportParser()\n",
    "\n",
    "# 2014년부터 2024년까지 반복\n",
    "for year in range(2014, 2025):\n",
    "    file_path = f'삼성전자_감사보고서_2014_2024/감사보고서_{year}.htm'\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        parsed_html = parser.parse_html(file_path)\n",
    "        \n",
    "        if parsed_html:\n",
    "            inventory_df = parser.extract_inventory_table(parsed_html)\n",
    "            \n",
    "            print(f\"\\n==================== {year}년 감사보고서 ====================\")\n",
    "            if inventory_df is not None:\n",
    "                display(inventory_df)\n",
    "                \n",
    "                # 추출된 DataFrame을 CSV 파일로 저장\n",
    "                csv_filename = f\"outputs/재고자산_{year}.csv\"\n",
    "                inventory_df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "                print(f\"'{csv_filename}' 파일로 저장되었습니다.\")\n",
    "            else:\n",
    "                print(f\"재고자산 테이블을 찾을 수 없습니다.\")\n",
    "    else:\n",
    "        print(f\"\\n파일을 찾을 수 없습니다: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = AuditReportParser()\n",
    "\n",
    "# 출력 파일을 저장할 디렉토리 생성\n",
    "if not os.path.exists('outputs'):\n",
    "    os.makedirs('outputs')\n",
    "\n",
    "for year in range(2014, 2025):\n",
    "    # 파일 경로 설정 (경로를 사용자의 환경에 맞게 수정해주세요)\n",
    "    file_path = f'삼성전자_감사보고서_2014_2024/감사보고서_{year}.htm'\n",
    "    # 파일 존재 여부 확인 후 파싱 및 출력\n",
    "    if os.path.exists(file_path):\n",
    "        parsed_html = parser.parse_html(file_path)\n",
    "        \n",
    "        if parsed_html:\n",
    "            df = parser.extract_investment_changes(parsed_html)\n",
    "            \n",
    "            print(f\"==================== {year}년 투자의 변동내역 파싱 결과 ====================\")\n",
    "            if df is not None:\n",
    "                # DataFrame 출력\n",
    "                display(df) \n",
    "                \n",
    "                # DataFrame을 CSV 파일로 저장\n",
    "                csv_filename = f\"outputs/2-a {year}.csv\"\n",
    "                df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "                print(f\"'{csv_filename}' 파일로 저장되었습니다.\")\n",
    "            else:\n",
    "                print(\"테이블을 찾을 수 없습니다.\")\n",
    "    else:\n",
    "        print(f\"파일을 찾을 수 없습니다: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = AuditReportParser()\n",
    "\n",
    "# 출력 파일을 저장할 디렉토리 생성\n",
    "if not os.path.exists('outputs'):\n",
    "    os.makedirs('outputs')\n",
    "\n",
    "for year in range(2014, 2025):\n",
    "    # 파일 경로 설정 (경로를 사용자의 환경에 맞게 수정해주세요)\n",
    "    file_path = f'삼성전자_감사보고서_2014_2024/감사보고서_{year}.htm'\n",
    "    # 파일 존재 여부 확인 후 파싱 및 출력\n",
    "    if os.path.exists(file_path):\n",
    "        parsed_html = parser.parse_html(file_path)\n",
    "\n",
    "    if parsed_html:\n",
    "        df = parser.extract_major_investments(parsed_html)\n",
    "        \n",
    "        print(f\"==================== {year}년 관계기업 투자 현황 파싱 결과 ====================\")\n",
    "        if df is not None:\n",
    "            # DataFrame 출력\n",
    "            display(df) \n",
    "            \n",
    "            # DataFrame을 CSV 파일로 저장\n",
    "            csv_filename = f\"outputs/2-b {year}.csv\"\n",
    "            df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "            print(f\"'{csv_filename}' 파일로 저장되었습니다.\")\n",
    "        else:\n",
    "            print(\"테이블을 찾을 수 없습니다.\")\n",
    "else:\n",
    "    print(f\"파일을 찾을 수 없습니다: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = AuditReportParser()\n",
    "\n",
    "# 출력 파일을 저장할 디렉토리 생성\n",
    "if not os.path.exists('outputs'):\n",
    "    os.makedirs('outputs')\n",
    "\n",
    "for year in range(2014, 2025):\n",
    "    # 파일 경로 설정 (경로를 사용자의 환경에 맞게 수정해주세요)\n",
    "    file_path = f'삼성전자_감사보고서_2014_2024/감사보고서_{year}.htm'\n",
    "    # 파일 존재 여부 확인 후 파싱 및 출력\n",
    "    if os.path.exists(file_path):\n",
    "        parsed_html = parser.parse_html(file_path)\n",
    "\n",
    "    if parsed_html:\n",
    "        df = parser.extract_subsidiaries(parsed_html)\n",
    "        \n",
    "        print(f\"==================== {year}년 종속기업 재무정보 파싱 결과 ====================\")\n",
    "        if df is not None:\n",
    "            # DataFrame 출력\n",
    "            display(df) \n",
    "            \n",
    "            # DataFrame을 CSV 파일로 저장\n",
    "            csv_filename = f\"outputs/2-c-1 {year}.csv\"\n",
    "            df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "            print(f\"'{csv_filename}' 파일로 저장되었습니다.\")\n",
    "        else:\n",
    "            print(\"테이블을 찾을 수 없습니다.\")\n",
    "else:\n",
    "    print(f\"파일을 찾을 수 없습니다: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 인스턴스 생성\n",
    "parser = AuditReportParser()\n",
    "\n",
    "# 출력 파일을 저장할 디렉토리 생성\n",
    "if not os.path.exists('outputs'):\n",
    "    os.makedirs('outputs')\n",
    "\n",
    "# 2014년부터 2024년까지 반복\n",
    "for year in range(2014, 2025):\n",
    "    # 파일 경로 설정 (경로를 사용자의 환경에 맞게 수정해주세요)\n",
    "    file_path = f'삼성전자_감사보고서_2014_2024/감사보고서_{year}.htm'\n",
    "    \n",
    "    # 파일명에서 연도 추출\n",
    "    match = re.search(r'(\\d{4})', os.path.basename(file_path))\n",
    "    if match:\n",
    "        current_year = int(match.group(1))\n",
    "    else:\n",
    "        current_year = None\n",
    "    \n",
    "    if os.path.exists(file_path) and current_year:\n",
    "        parsed_html = parser.parse_html(file_path)\n",
    "        \n",
    "        if parsed_html:\n",
    "            df = parser.extract_financial_info(parsed_html,current_year)\n",
    "            \n",
    "            print(f\"==================== {current_year}년 관계기업 재무정보 파싱 결과 ====================\")\n",
    "            if df is not None:\n",
    "                display(df) \n",
    "                csv_filename = f\"outputs/2-d_{current_year}.csv\"\n",
    "                df.to_csv(csv_filename, index=True, encoding='utf-8-sig')\n",
    "                print(f\"'{csv_filename}' 파일로 저장되었습니다.\")\n",
    "            else:\n",
    "                print(\"테이블을 찾을 수 없습니다.\")\n",
    "    else:\n",
    "        print(f\"\\n파일을 찾을 수 없습니다: {file_path}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = AuditReportParser()\n",
    "\n",
    "# 출력 파일을 저장할 디렉토리 생성\n",
    "if not os.path.exists('outputs'):\n",
    "    os.makedirs('outputs')\n",
    "\n",
    "for year in range(2014, 2025):\n",
    "    # 파일 경로 설정 (경로를 사용자의 환경에 맞게 수정해주세요)\n",
    "    file_path = f'삼성전자_감사보고서_2014_2024/감사보고서_{year}.htm'\n",
    "    # 파일 존재 여부 확인 후 파싱 및 출력\n",
    "    if os.path.exists(file_path):\n",
    "        parsed_html = parser.parse_html(file_path)\n",
    "\n",
    "    if parsed_html:\n",
    "        print(f\"==================== {year}년 관계기업 투자주식의 내역 파싱 결과 ====================\")\n",
    "        df = parser.extract_specific_investment_table(parsed_html)\n",
    "            \n",
    "        if df is not None:\n",
    "            display(df)\n",
    "            csv_filename = f\"outputs/2-e {year}.csv\"\n",
    "            df.to_csv(csv_filename, index=True, encoding='utf-8-sig')\n",
    "            print(f\"'{csv_filename}' 파일로 저장되었습니다.\")\n",
    "        else:\n",
    "            print(\"테이블을 찾을 수 없습니다.\")\n",
    "else:\n",
    "    print(f\"파일을 찾을 수 없습니다: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = AuditReportParser()\n",
    "\n",
    "# 출력 파일을 저장할 디렉토리 생성\n",
    "if not os.path.exists('outputs'):\n",
    "    os.makedirs('outputs')\n",
    "\n",
    "for year in range(2014, 2025):\n",
    "    # 파일 경로 설정 (경로를 사용자의 환경에 맞게 수정해주세요)\n",
    "    file_path = f'삼성전자_감사보고서_2014_2024/감사보고서_{year}.htm'\n",
    "    # 파일 존재 여부 확인 후 파싱 및 출력\n",
    "    if os.path.exists(file_path):\n",
    "        parsed_html = parser.parse_html(file_path)\n",
    "\n",
    "    if parsed_html:\n",
    "        print(f\"==================== {year}년 유형자산 변동내역 파싱 결과 ====================\")\n",
    "        df = parser.extract_tangible_assets_table(parsed_html)\n",
    "            \n",
    "        if df is not None:\n",
    "            display(df)\n",
    "            csv_filename = f\"outputs/3-a {year}.csv\"\n",
    "            df.to_csv(csv_filename, index=True, encoding='utf-8-sig')\n",
    "            print(f\"'{csv_filename}' 파일로 저장되었습니다.\")\n",
    "        else:\n",
    "            print(\"테이블을 찾을 수 없습니다.\")\n",
    "else:\n",
    "    print(f\"파일을 찾을 수 없습니다: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = AuditReportParser()\n",
    "\n",
    "# 출력 파일을 저장할 디렉토리 생성\n",
    "if not os.path.exists('outputs'):\n",
    "    os.makedirs('outputs')\n",
    "\n",
    "for year in range(2014, 2025):\n",
    "    # 파일 경로 설정 (경로를 사용자의 환경에 맞게 수정해주세요)\n",
    "    file_path = f'삼성전자_감사보고서_2014_2024/감사보고서_{year}.htm'\n",
    "    # 파일 존재 여부 확인 후 파싱 및 출력\n",
    "    if os.path.exists(file_path):\n",
    "        parsed_html = parser.parse_html(file_path)\n",
    "\n",
    "    if parsed_html:\n",
    "        print(f\"==================== {year}년 무형자산 변동내역 파싱 결과 ====================\")\n",
    "        df = parser.extract_intangible_assets_table(parsed_html)\n",
    "            \n",
    "        if df is not None:\n",
    "            display(df)\n",
    "            csv_filename = f\"outputs/3-b {year}.csv\"\n",
    "            df.to_csv(csv_filename, index=True, encoding='utf-8-sig')\n",
    "            print(f\"'{csv_filename}' 파일로 저장되었습니다.\")\n",
    "        else:\n",
    "            print(\"테이블을 찾을 수 없습니다.\")\n",
    "else:\n",
    "    print(f\"파일을 찾을 수 없습니다: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
